{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to News Media Bias Plus Project","text":""},{"location":"#our-mission","title":"Our Mission","text":"<p>This project is dedicated to promoting responsible AI development and addressing critical challenges in artificial intelligence, with a special focus on media bias and disinformation. We explore the intersection of AI safety and media integrity, focusing on:</p> <ul> <li>Bias Detection: Uncovering and mitigating biases in AI systems and media content.</li> <li>Disinformation Challenges: Addressing misinformation and disinformation and its societal impact.</li> <li>Ethical AI: Promoting responsible use of AI in news reporting and production.</li> </ul>"},{"location":"#our-framework","title":"Our Framework","text":"<p>Using state-of-the-art AI methods, we analyze news articles or documents or images to detect and categorize different types of media bias. Our system examines:</p> <ul> <li>Topic coverage and framing</li> <li>Idealogical leanings and sentiment</li> <li>Language patterns and tone</li> <li>Source credibility and transparency</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Bias Analysis: Compare coverage of specific topics across multiple news articles.</li> <li>AI Safety Metrics: Track the use of AI in document analysis and its impact on bias.</li> <li>Disinformation Alerts: Detect and flag potential AI-generated fake news or deepfakes, whether in text or images.</li> <li>Interactive Visualizations: Explore media bias trends and AI influence in journalism.</li> </ul>"},{"location":"#why-it-matters","title":"Why It Matters","text":"<p>Understanding the role of AI in media bias and disinformation is crucial for:</p> <ul> <li>Promoting media literacy in the age of AI.</li> <li>Ensuring responsible AI development in journalism.</li> <li>Fostering trust in both AI systems and media institutions.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>AI Safety Fundamentals</li> <li>Dataset</li> <li>Spotting AI-Generated Content</li> </ul>"},{"location":"#get-involved","title":"Get Involved","text":"<ul> <li>Contribute to our bias and disinformation detection efforts</li> </ul>"},{"location":"#contact-us","title":"Contact Us","text":"<ul> <li>Contact form or contact information</li> </ul>"},{"location":"About-us/","title":"About Us","text":""},{"location":"About-us/#mission-statement","title":"Mission Statement","text":""},{"location":"About-us/#the-team","title":"The Team","text":""},{"location":"About-us/#vision-for-the-future","title":"Vision for the Future","text":"<p>Discuss what the future holds for your project, including upcoming goals, projects, and expansions.</p>"},{"location":"Annotation/","title":"Annotation Framework","text":""},{"location":"Annotation/#1-annotation-guidelines-and-procedure","title":"1. Annotation Guidelines and Procedure","text":"<p>This framework outlines a structured approach for annotating news articles, incorporating both text and images. The process begins with human annotators labeling a carefully selected subset of the data. Once this subset is annotated, Large Language Models (LLMs) take over to expand these labels across the entire dataset. By aligning annotations with corresponding text and images, the result is a Silver Standard Dataset.</p>"},{"location":"Annotation/#2-quality-control","title":"2. Quality Control","text":"<p>To ensure the reliability and consistency of annotations, multiple quality control mechanisms are in place. Cohen's Kappa is employed to measure inter-annotator agreement, highlighting areas that may require further clarification. In addition to automated checks, human reviewers manually evaluate a portion of the annotations. This dual approach maintains the quality and accuracy of the labeled data.</p>"},{"location":"Annotation/#3-evaluation-pipeline","title":"3. Evaluation Pipeline","text":"<p>The evaluation process is designed to convert the Silver Standard Dataset into a Gold Standard Dataset. Initially, an LLM-based jury provides judgments on the quality of the annotations. These judgments are then reviewed by human experts, who validate, refine, or discard annotations as necessary. This collaborative effort between machines and human reviewers ensures a high-quality final dataset.</p>"},{"location":"Annotation/#4-system-training-and-testing","title":"4. System Training and Testing","text":"<p>Once the Gold Standard Dataset is established, it serves as the foundation for training and testing models in multi-modal bias detection. This process ensures the model\u2019s performance remains robust across various data types, including both text and images.</p>"},{"location":"Benchmark/","title":"Benchmarking for Annotation Framework","text":""},{"location":"Benchmark/#purpose","title":"Purpose","text":"<p>The purpose of this benchmarking page is to evaluate the performance of Small Language Models (SLMs) and Large Language Models (LLMs) in our annotation framework. In this context, we refer to SLMs as those with fewer parameters, typically less than 15 million, such as BERT and GPT-2. LLMs, like Llama3, Mistral, Gemma, Phi, have significantly more parameters, often in the hundreds of millions to billions. This relative difference in scale allows us to compare the efficiency to handle more complex tasks and datasets, while SLMs are more efficient for simpler tasks or environments with limited resources.</p>"},{"location":"Benchmark/#benchmarking-on-texts","title":"Benchmarking on Texts","text":""},{"location":"Benchmark/#small-language-models-slms","title":"Small Language Models (SLMs)","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) BERT-base-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BERT-large-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A DistilBERT Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A RoBERTa-base Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A GPT2 Fine-tuning Decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BART Fine-tuning Encoder-decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A"},{"location":"Benchmark/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"Benchmark/#llama-models","title":"Llama Models","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Llama 3.1-8B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.1-8B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B-Instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-3B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-8B-sequence classifier Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Llama 3 (70B) N/A N/A N/A 1900"},{"location":"Benchmark/#other-llms","title":"Other LLMs","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Mistral-v0.3-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias, Real/Unbias N/A Mistral-v0.3 Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Mistral-large-instruct (IFT) IFT N/A Fake/Bias, Real/Unbias N/A Gemma-2-9b-Instruct 0-shot, 5-shot, IFT Decoder-only, Causal LM Fake/Bias, Real/Unbias N/A Gemma-2-9b Fine-tuning Decoder-only, sequence classification Fake/Bias, Real/Unbias N/A"},{"location":"Benchmark/#benchmarking-on-multimodality","title":"Benchmarking on Multimodality","text":""},{"location":"Benchmark/#small-language-models-slms_1","title":"Small Language Models (SLMs)","text":"Model Training Method Architecture BERT + ResNet-34 Fine-tuning Encoder-encoder SAFE (Text-CNN + Image2Sentence) Fine-tuning Encoder-encoder SpotFake (XLNET + VGG-19) Fine-tuning Encoder-encoder MCAN (BERT + VGG-19/CNN) Fine-tuning Encoder-encoder FND-CLIP (BERT/ResNet + CLIP) Fine-tuning Encoder-encoder InstructBlipV Fine-tuning Encoder-encoder DistilBERT + CLIP Fine-tuning Encoder-encoder"},{"location":"Benchmark/#large-language-models-llms_1","title":"Large Language Models (LLMs)","text":"Model Training Method Architecture google/paligemma-3b-pt-224 Instruction fine-tuning Decoder-encoder microsoft/Phi-3-vision-128k-instruct 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder Pixtral-12B-2409 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder LLaVA-1.6 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder Llama-3.2-11B-Vision-Instruct 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder meta-llama/Llama-3.2-11B-Vision Fine-tuning Decoder-encoder meta-llama/Llama-Guard-3-11B-Vision Inference Decoder-encoder"},{"location":"Benchmarks/","title":"Benchmarking for Annotation Framework","text":""},{"location":"Benchmarks/#purpose","title":"Purpose","text":"<p>The purpose of this benchmarking page is to evaluate the performance of Small Language Models (SLMs) and Large Language Models (LLMs) in our annotation framework. In this context, we refer to SLMs as those with fewer parameters, typically less than 15 million, such as BERT and GPT-2. LLMs, like Llama3, Mistral, Gemma, Phi, have significantly more parameters, often in the hundreds of millions to billions. This relative difference in scale allows us to compare the efficiency to handle more complex tasks and datasets, while SLMs are more efficient for simpler tasks or environments with limited resources.</p>"},{"location":"Benchmarks/#benchmarking-on-texts","title":"Benchmarking on Texts","text":""},{"location":"Benchmarks/#model-comparison","title":"Model Comparison","text":""},{"location":"Benchmarks/#small-language-models-slms","title":"Small Language Models (SLMs)","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) BERT-base-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BERT-large-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A DistilBERT Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A RoBERTa-base Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A GPT2 Fine-tuning Decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BART Fine-tuning Encoder-decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A"},{"location":"Benchmarks/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"Benchmarks/#llama-models","title":"Llama Models","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Llama 3.1-8B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.1-8B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B-Instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-3B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-8B-sequence classifier Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Llama 3 (70B) N/A N/A N/A 1900"},{"location":"Benchmarks/#other-llms","title":"Other LLMs","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Mistral-v0.3-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias, Real/Unbias N/A Mistral-v0.3 Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Mistral-large-instruct (IFT) IFT N/A Fake/Bias, Real/Unbias N/A Gemma-2-9b-Instruct 0-shot, 5-shot, IFT Decoder-only, Causal LM Fake/Bias, Real/Unbias N/A Gemma-2-9b Fine-tuning Decoder-only, sequence classification Fake/Bias, Real/Unbias N/A GPT-3 (175B) N/A N/A N/A 502"},{"location":"Contact-Us/","title":"Contact Us","text":""},{"location":"Contact-Us/#general-inquiries","title":"General Inquiries","text":""},{"location":"Contact-Us/#feedback","title":"Feedback","text":""},{"location":"FAQs/","title":"Frequently Asked Questions","text":""},{"location":"FAQs/#general-questions","title":"General Questions","text":"<p>Q1: What is the main focus of your project?</p> <p>Q2: How can I access your dataset?</p>"},{"location":"FAQs/#technical-questions","title":"Technical Questions","text":"<p>Q3: What tools do I need to use your dataset?</p> <p>Q4: Who should I contact for technical support?</p>"},{"location":"FAQs/#contribution-questions","title":"Contribution Questions","text":"<p>Q5: How can I contribute to your project?</p> <p>Q6: Are there any specific guidelines for contributors?</p>"},{"location":"FAQs/#ethical-and-privacy-questions","title":"Ethical and Privacy Questions","text":"<p>Q7: How do you ensure the privacy and ethical use of your data?</p> <p>Q8: What measures are in place to prevent bias in your AI models?</p>"},{"location":"Publications/","title":"Publications","text":""},{"location":"Publications/#journal-articles","title":"Journal Articles","text":""},{"location":"Publications/#conference-papers","title":"Conference Papers","text":""},{"location":"Publications/#media-coverage","title":"Media Coverage","text":"<p>List instances of media coverage, including articles, interviews, and mentions in popular press. Provide links and a brief description of each piece.</p>"},{"location":"dataset.md/","title":"Dataset","text":""},{"location":"dataset.md/#overview","title":"Overview","text":"<p>Describe the methodologies used for data collection, including any automated scraping, partnerships with news organizations, or crowdsourced methods.</p>"},{"location":"dataset.md/#data-structure","title":"Data Structure","text":"<p>Detail the structure of your dataset, including any categorizations, annotations, and how the data is formatted.</p>"},{"location":"dataset.md/#access","title":"Access","text":"<p>Provide instructions on how users can access the dataset, including any requirements or limitations.</p>"},{"location":"dataset.md/#sample-data","title":"Sample Data","text":""}]}