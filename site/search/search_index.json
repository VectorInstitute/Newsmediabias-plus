{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"News Media Bias Plus Project","text":""},{"location":"#our-mission","title":"Our Mission","text":"<p>The News Media Bias Plus Project leads the way in responsible AI research, advancing understanding of media bias and disinformation through artificial intelligence. We focus on the critical intersection of AI safety and media integrity, aiming to foster a more balanced and transparent information ecosystem.</p> <p>Our key focus areas include:</p> <ul> <li>Bias Detection: Identifying and mitigating biases in AI systems and media content.  </li> <li>Disinformation Challenges: Tackling misinformation and its societal impacts.  </li> <li>Ethical AI: Promoting responsible AI use in media and journalism.</li> </ul>"},{"location":"#our-framework","title":"Our Framework","text":"<p>Leveraging cutting-edge AI techniques, we analyze diverse media formats\u2014news articles, documents, and images\u2014to detect, categorize, and mitigate media bias. Our comprehensive system evaluates:</p> <ul> <li>Topic Coverage &amp; Framing: How outlets prioritize and present topics.  </li> <li>Ideological Leanings &amp; Sentiment: Underlying political tones and biases.  </li> <li>Language Patterns &amp; Tone: Stylistic features shaping perception.  </li> <li>Source Credibility &amp; Transparency: Trustworthiness and openness of sources.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Bias Analysis: Cross-source comparative analyses highlighting bias differences.  </li> <li>AI Safety Metrics: Monitoring AI\u2019s role in content creation and bias detection.  </li> <li>Disinformation Alerts: Identifying AI-generated disinformation like deepfakes and fake news.  </li> <li>Interactive Visualizations: Tools to explore media bias trends and AI\u2019s impact on journalism.</li> </ul>"},{"location":"#why-it-matters","title":"Why It Matters","text":"<p>In an era of growing AI influence on media, understanding AI's role in bias and disinformation is essential for:</p> <ul> <li>Promoting Media Literacy: Empowering critical evaluation of news.  </li> <li>Ensuring Responsible AI Development: Advocating ethical AI use in journalism.  </li> <li>Building Trust: Enhancing transparency and accountability in media and AI systems.</li> </ul>"},{"location":"#dataset-access","title":"Dataset Access","text":"<p>Explore all our datasets, models, and resources on the dedicated Datasets page.</p> <p>For quick access, visit the NewsMediaBias-Plus dataset on Hugging Face.</p>"},{"location":"#get-involved-contact-us","title":"Get Involved &amp; Contact Us","text":"<p>We welcome researchers, developers, and the public to join us in combating media bias and disinformation. Support our work by:</p> <ul> <li>Collaborating on research and development.  </li> <li>Requesting Data Access or providing feedback through the form below.</li> </ul> Loading\u2026"},{"location":"#contact-information","title":"Contact Information","text":"<p>Vector Institute for Artificial Intelligence Schwartz Reisman Innovation Campus 108 College St., Suite W1140 Toronto, ON M5G 0C6  </p> <p>Email: shaina.raza@vectorinstitute.ai</p>"},{"location":"About-us/","title":"Team","text":""},{"location":"About-us/#project-lead","title":"Project Lead","text":"Dr. Shaina RazaApplied Machine Learning Scientist, Responsible AI"},{"location":"About-us/#contributors","title":"Contributors","text":"Aravind Narayanan Applied ML Intern, Vector Institute    Vahid Reza Khazaie Applied ML Specialist, Vector Institute    Mukund Sayeeganesh Chettiar  Applied ML Intern, Vector Institute    Marcelo Lotif Senior Software Developer &amp; ML Engineer, Vector Institute    Emrul Hasan Ph.D. Candidate, Toronto Metropolitan University, Vector Institute    Veronica Chatrath Technical Program Manager, Vector Institute    Franklin Ogidi Applied Machine Learning Specialist, Vector Institute    Caesar Saleh Intern, University of Toronto, Vector Institute    Roya Javadi Machine Learning Software Developer, Vector Institute    Sina Salimian Research Assistant, University of Calgary    Maximus Powers Ethical Spectacle Research    Mark Coatsworth Vector Institute"},{"location":"About-us/#advisors","title":"Advisors","text":"Dr. Arash Afkanpour  Vector Institute    Dr. Gias Uddin Professor, York University     Aditya Jain Applied Research Scientist, Meta"},{"location":"About-us/#acknowledgments","title":"Acknowledgments","text":"<p>We extend our sincere thanks to Michael Joseph, Manoj Athreya, Sherry, Sara Kodeiri, Roya Javadi, Fatemeh Tavakoli, Nan Ajmain, Wu Rupert, and the entire team for their valuable assistance in reviewing the data.</p>"},{"location":"Annotation/","title":"Annotation Framework","text":""},{"location":"Annotation/#1-annotation-guidelines-and-procedure","title":"1. Annotation Guidelines and Procedure","text":"<p>This framework outlines a structured approach for annotating news articles, incorporating both text and images. The process begins with human annotators labeling a carefully selected subset of the data. Once this subset is annotated, Large Language Models (LLMs) take over to expand these labels across the entire dataset. By aligning annotations with corresponding text and images, the result is a Silver Standard Dataset.</p>"},{"location":"Annotation/#2-quality-control","title":"2. Quality Control","text":"<p>To ensure the reliability and consistency of annotations, multiple quality control mechanisms are in place. Cohen's Kappa is employed to measure inter-annotator agreement, highlighting areas that may require further clarification. In addition to automated checks, human reviewers manually evaluate a portion of the annotations. This dual approach maintains the quality and accuracy of the labeled data.</p>"},{"location":"Annotation/#3-evaluation-pipeline","title":"3. Evaluation Pipeline","text":"<p>The evaluation process is designed to convert the Silver Standard Dataset into a Gold Standard Dataset. Initially, an LLM-based jury provides judgments on the quality of the annotations. These judgments are then reviewed by human experts, who validate, refine, or discard annotations as necessary. This collaborative effort between machines and human reviewers ensures a high-quality final dataset.</p>"},{"location":"Annotation/#4-system-training-and-testing","title":"4. System Training and Testing","text":"<p>Once the Gold Standard Dataset is established, it serves as the foundation for training and testing models in multi-modal bias detection. This process ensures the model\u2019s performance remains robust across various data types, including both text and images.</p>"},{"location":"Benchmark/","title":"Benchmarking Annotation Framework","text":""},{"location":"Benchmark/#overview","title":"Overview","text":"<p>This page presents the performance benchmarking of Small Language Models (SLMs) and Large Language Models (LLMs) within our annotation framework. The objective is to evaluate how these models perform in tasks involving text and multimodal data (text + image). For this benchmarking, SLMs are defined as models with fewer parameters, typically below 15 million, such as BERT and GPT-2. In contrast, LLMs\u2014including models like Llama3, Mistral, Gemma, and Phi\u2014possess hundreds of millions to billions of parameters. This scale difference highlights the trade-off between efficiency and complexity when handling various tasks and datasets.</p>"},{"location":"Benchmark/#benchmarking-results-text-based-models","title":"Benchmarking Results: Text-Based Models","text":"Model Configuration Precision Recall F1 Test Accuracy Small Language Models BERT-base-uncased FT 0.8887 0.8870 0.8878 0.8870 DistilBERT FT 0.8665 0.8554 0.8609 0.8710 RoBERTa-base FT 0.8940 0.8940 0.8940 0.8940 GPT2 FT 0.8762 0.8751 0.8756 0.8751 BART FT 0.8762 0.8760 0.8761 0.8760 Large Language Models Llama 3.1-8B-instruct 0-shot 0.8280 0.6890 0.7521 0.7200 5-shot 0.8400 0.7700 0.8035 0.7905 IFT 0.8019 0.8019 0.8019 0.8180 Llama 3.1-8B (base) FT 0.8800 0.8600 0.8699 0.8320 Llama 3.2-3B-instruct 0-shot 0.7386 0.7550 0.7467 0.6897 5-shot 0.7989 0.6840 0.7370 0.6133 IFT 0.8390 0.7984 0.8182 0.8084 Llama 3.2-3B (base) FT 0.8400 0.8500 0.8450 0.8200 Mistral-v0.3 7B-instruct 0-shot 0.8153 0.5250 0.6387 0.6990 5-shot 0.8319 0.8134 0.8225 0.7830 IFT 0.8890 0.9240 0.9062 0.7980 Mistral-v0.3 7B (base) FT 0.8200 0.7400 0.7779 0.8014 Qwen2.5-7B 0-shot 0.8576 0.8576 0.8576 0.8576 5-shot 0.8660 0.8790 0.8724 0.8900 IFT 0.8357 0.8474 0.8415 0.8474 <p>Table 1: Performance metrics for various language models and configurations. Configuration types: 0-shot = No prior examples used for inference, 5-shot = Five examples provided for context before inference, FT = Fine-tuned on task-specific data, IFT = Instruction fine-tuned with targeted training.</p> Model Config. (Text-Image) Precision Recall F1 Test Accuracy Small Language Models SpotFake (XLNET + VGG-19) FT 0.7415 0.6790 0.7089 0.8151 BERT + ResNet-34 FT 0.8311 0.6277 0.7152 0.8248 FND-CLIP (BERT and CLIP) FT 0.6935 0.7151 0.7041 0.8971 Distill-RoBERTa and CLIP FT 0.7000 0.6600 0.6794 0.8600 Large Vision Language Models Phi-3-vision-128k-instruct 0-shot 0.7400 0.6700 0.7033 0.7103 Phi-3-vision-128k-instruct 5-shot 0.7600 0.7200 0.7395 0.7024 Phi-3-vision-128k-instruct IFT 0.7800 0.8000 0.7899 0.7200 LLaVA-1.6 0-shot 0.7531 0.6466 0.6958 0.6500 LLaVA-1.6 5-shot 0.7102 0.6893 0.6996 0.6338 Llama-3.2-11B-Vision-Instruct 0-shot 0.6668 0.7233 0.6939 0.7060 Llama-3.2-11B-Vision-Instruct 5-shot 0.7570 0.7630 0.7600 0.7299 Llama-3.2-11B-Vision-Instruct IFT 0.7893 0.8838 0.8060 0.9040 <p>Table 2: Performance metrics for various small and large language models in text-image configurations. Configuration types: 0-shot = No prior examples used for inference, 5-shot = Five examples provided for context before inference, FT = Fine-tuning, IFT = Instruction Fine-tuning.</p> <p>This benchmarking offers an insightful overview of how various models, ranging from smaller to large-scale, perform in distinct environments and tasks. The text-based and multimodal benchmarks reflect the strength of these models in handling the complexities of both textual data and combined text-image inputs, providing a useful reference for selecting the appropriate model based on the task requirements.</p>"},{"location":"Fairsense/","title":"FairSense-AI","text":"<p>Fairsense-AI is a state of the art AI-driven platform designed to promote transparency, fairness, and equity by analyzing bias in textual and visual content. Built with sustainability in mind, it leverages energy-efficient AI frameworks to ensure an eco-friendly approach to tackling societal biases.</p> <p>FairSense-AI on PyPI </p> <p>Quick check application on Google Colab T4</p>"},{"location":"Fairsense/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"Fairsense/#step-1-install-fair-sense-ai","title":"Step 1: Install Fair-Sense-AI","text":"<p>Install the Fair-Sense-AI package using pip:</p> <pre><code>pip install Fair-Sense-AI\n</code></pre>"},{"location":"Fairsense/#step-2-quickstart-code-examples","title":"Step 2: Quickstart Code Examples","text":""},{"location":"Fairsense/#1-text-bias-analysis","title":"1. Text Bias Analysis","text":"<pre><code>from fairsenseai import analyze_text_for_bias\n\n# Example input text to analyze for bias\ntext_input = \"Men are naturally better at decision-making, while women excel at emotional tasks.\"\n\n# Analyze the text for bias\nhighlighted_text, detailed_analysis, bias_score = analyze_text_for_bias(text_input)\n\n# Print the analysis results\nprint(\"Highlighted Text:\", highlighted_text)\nprint(\"Detailed Analysis:\", detailed_analysis)\nprint(\"Bias Score:\", bias_score)\n</code></pre>"},{"location":"Fairsense/#2-image-bias-analysis","title":"2. Image Bias Analysis","text":"<pre><code>import requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom fairsenseai import analyze_image_for_bias\n\n# URL of the image to analyze\nimage_url = \"https://media.top1000funds.com/wp-content/uploads/2019/12/iStock-525807555.jpg\"\n\n# Fetch and load the image\nresponse = requests.get(image_url)\nimage = Image.open(BytesIO(response.content))\n\n# Analyze the image for bias\nhighlighted_caption, image_analysis = analyze_image_for_bias(image)\n\n# Print the analysis results\nprint(\"Highlighted Caption:\", highlighted_caption)\nprint(\"Image Analysis:\", image_analysis)\n</code></pre>"},{"location":"Fairsense/#3-launch-the-interactive-application","title":"3. Launch the Interactive Application","text":"<pre><code>from fairsenseai import main\n\n# Launch the Gradio application (will open in the browser)\nmain()\n</code></pre>"},{"location":"Fairsense/#bias-detection-tutorial","title":"Bias Detection Tutorial","text":""},{"location":"Fairsense/#data-and-sample-notebooks","title":"Data and Sample Notebooks","text":"<ol> <li> <p>Download the Data: Google Drive Link</p> </li> <li> <p>Colab Notebook: Run the Tutorial</p> </li> </ol>"},{"location":"Fairsense/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Python 3.7+    Ensure Python is installed. Download it here.</p> </li> <li> <p>Tesseract OCR    Required for extracting text from images.</p> </li> </ol> <p>#### Installation Instructions:    - Ubuntu:      <code>bash      sudo apt-get update      sudo apt-get install tesseract-ocr</code>    - macOS (Homebrew):      <code>bash      brew install tesseract</code>    - Windows:      Download and install Tesseract OCR from this link.</p> <ol> <li>Optional (GPU Acceleration)    Install PyTorch with CUDA support:</li> </ol> <p><code>bash    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117</code></p>"},{"location":"Fairsense/#usage-instructions","title":"Usage Instructions","text":""},{"location":"Fairsense/#launching-the-application","title":"Launching the Application","text":"<p>Run the following command to start Fair-Sense-AI:</p> <pre><code>Fair-Sense-AI\n</code></pre> <p>This will launch the Gradio-powered interface in your default web browser.</p>"},{"location":"Fairsense/#features","title":"Features","text":""},{"location":"Fairsense/#1-text-analysis","title":"1. Text Analysis","text":"<ul> <li>Input or paste text in the Text Analysis tab.</li> <li>Click Analyze to detect and highlight biases.</li> </ul>"},{"location":"Fairsense/#2-image-analysis","title":"2. Image Analysis","text":"<ul> <li>Upload an image in the Image Analysis tab.</li> <li>Click Analyze to detect biases in embedded text or captions.</li> </ul>"},{"location":"Fairsense/#3-batch-text-csv-analysis","title":"3. Batch Text CSV Analysis","text":"<ul> <li>Upload a CSV file with a <code>text</code> column in the Batch Text CSV Analysis tab.</li> <li>Click Analyze CSV to process all entries.</li> </ul>"},{"location":"Fairsense/#4-batch-image-analysis","title":"4. Batch Image Analysis","text":"<ul> <li>Upload multiple images in the Batch Image Analysis tab.</li> <li>Click Analyze Images for a detailed review.</li> </ul>"},{"location":"Fairsense/#5-ai-governance-insights","title":"5. AI Governance Insights","text":"<ul> <li>Navigate to the AI Governance and Safety tab.</li> <li>Choose a predefined topic or input your own query.</li> <li>Click Get Insights for recommendations.</li> </ul>"},{"location":"Fairsense/#additional-setup-in-colab","title":"Additional Setup in Colab","text":"<p>Run the following commands to ensure everything is ready:</p> <pre><code>!pip install --quiet fair-sense-ai\n!pip uninstall sympy -y\n!pip install sympy --upgrade\n!apt update\n!apt install -y tesseract-ocr\n</code></pre> <p>Note: Restart your system if you're using Google Colab.</p>"},{"location":"Fairsense/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Slow Model Download:   Ensure a stable internet connection for downloading models.</p> </li> <li> <p>Tesseract OCR Errors:   Verify Tesseract is installed and accessible in your system's PATH.</p> </li> <li> <p>GPU Support:   Use the CUDA-compatible version of PyTorch for better performance.</p> </li> </ul>"},{"location":"Fairsense/#contact","title":"Contact","text":"<p>For inquiries or support, contact: Shaina Raza, PhD Applied ML Scientist, Responsible AI shaina.raza@vectorinstitute.ai</p>"},{"location":"Fairsense/#license","title":"License","text":"<p>This project is licensed under the Creative Commons License.</p>"},{"location":"HumaniBench/","title":"HumaniBench: A Human-Centric Benchmark for Large Multimodal Models Evaluation","text":"<p>   \ud83c\udf10 Website \u00a0|\u00a0   \ud83d\udcc4 Paper \u00a0|\u00a0   \ud83d\udcca Dataset </p>"},{"location":"HumaniBench/#overview","title":"\ud83e\udde0 Overview","text":"<p>As multimodal generative AI systems become increasingly integrated into human-centered applications, evaluating their alignment with human values has become critical.</p> <p>HumaniBench is the first comprehensive benchmark designed to evaluate Large Multimodal Models (LMMs) on seven Human-Centered AI (HCAI) principles:</p> <ul> <li>Fairness</li> <li>Ethics</li> <li>Understanding</li> <li>Reasoning</li> <li>Language Inclusivity</li> <li>Empathy</li> <li>Robustness</li> </ul> <p></p>"},{"location":"HumaniBench/#features","title":"\ud83d\udce6 Features","text":"<ul> <li>\ud83d\udcf7 32,000+ Real-World Image\u2013Question Pairs</li> <li>\u2705 Human-Verified Ground Truth Annotations</li> <li>\ud83c\udf10 Multilingual QA Support (10+ languages)</li> <li>\ud83e\udde0 Open and Closed-Ended VQA Formats</li> <li>\ud83e\uddea Visual Robustness &amp; Bias Stress Testing</li> <li>\ud83d\udcd1 Chain-of-Thought Reasoning + Perceptual Grounding</li> </ul>"},{"location":"HumaniBench/#evaluation-tasks-overview","title":"\ud83d\udcc2 Evaluation Tasks Overview","text":"Task Focus Task 1: Scene Understanding Visual reasoning + bias/toxicity analysis in social attributes (gender, age, occupation, etc.) Task 2: Instance Identity Visual reasoning in culturally rich, socially grounded settings Task 3: Multiple Choice QA Structured attribute recognition via multi-choice questions Task 4: Multilingual Visual QA VQA across 10+ languages, including low-resource ones Task 5: Visual Grounding Bounding box localization of socially salient regions Task 6: Empathetic Captioning Human-style emotional captioning evaluation Task 7: Image Resilience Robustness testing via image perturbations"},{"location":"HumaniBench/#pipeline","title":"\ud83e\uddec Pipeline","text":"<p>Three-stage process:</p> <ol> <li> <p>Data Collection: Curated from global news imagery, tagged by social attributes (age, gender, race, occupation, sport)</p> </li> <li> <p>Annotation: GPT-4o\u2013assisted labeling + human expert verification</p> </li> <li> <p>Evaluation: Comprehensive scoring across Accuracy, Fairness, Robustness, Empathy and Faithfulness</p> </li> </ol>"},{"location":"HumaniBench/#key-insights","title":"\ud83d\udd11 Key Insights","text":"<ul> <li>\ud83d\udd0d Bias persists, especially across gender and race</li> <li>\ud83c\udf10 Multilingual gaps affect low-resource language performance</li> <li>\u2764\ufe0f Empathy and ethics vary significantly by model family</li> <li>\ud83e\udde0 Chain-of-Thought reasoning improves performance but doesn\u2019t fully mitigate bias</li> <li>\ud83e\uddea Robustness tests reveal fragility to noise, occlusion, and blur</li> </ul>"},{"location":"HumaniBench/#citation","title":"\ud83d\udcda Citation","text":"<p>If you use HumaniBench or this evaluation suite in your work, please cite:</p> <pre><code>@misc{raza2025humanibenchhumancentricframeworklarge,\n        title={HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation}, \n        author={Shaina Raza and Aravind Narayanan and Vahid Reza Khazaie and Ashmal Vayani and Mukund S. Chettiar and Amandeep Singh and Mubarak Shah and Deval Pandya},\n        year={2025},\n        eprint={2505.11454},\n        archivePrefix={arXiv},\n        primaryClass={cs.CV},\n        url={https://arxiv.org/abs/2505.11454}, \n  }\n\n</code></pre>"},{"location":"HumaniBench/#contact","title":"\ud83d\udcec Contact","text":"<p>For questions, collaborations, or dataset access requests, please contact the corresponding author at shaina.raza@vectorinstitute.ai.</p>"},{"location":"HumaniBench/#humanibench-promotes-trustworthy-fair-and-human-centered-multimodal-ai","title":"\u26a1 HumaniBench promotes trustworthy, fair, and human-centered multimodal AI.","text":"<p>We invite researchers, developers, and policymakers to explore, evaluate, and extend HumaniBench. \ud83d\ude80</p>"},{"location":"Publications/","title":"Publications","text":"<ul> <li> <p>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation   Raza, Shaina, Aravind Narayanan, Vahid Reza Khazaie, Ashmal Vayani, Mukund S. Chettiar, Amandeep Singh, Mubarak Shah, and Deval Pandya. arXiv preprint arXiv:2505.11454 (2025).</p> </li> <li> <p>Vldbench: Vision Language Models Disinformation Detection Benchmark   Raza, Shaina, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi et al. arXiv preprint arXiv:2502.11361 (2025).</p> </li> <li> <p>FairSense-AI: Responsible AI Meets Sustainability (Project Website)   Raza, Shaina, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, and Marcelo Lotif. arXiv preprint arXiv:2503.02865 (2025).</p> </li> <li> <p>Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs   Salimian, Sina, Gias Uddin, Most Husne Jahan, and Shaina Raza. arXiv preprint arXiv:2502.07186 (2025).</p> </li> <li> <p>Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods   Raza, Shaina, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, and Christos Emmanouilidis. arXiv preprint arXiv:2505.17870 (2025).</p> </li> <li> <p>Vilbias: A Framework for Bias Detection Using Linguistic and Visual Cues   Raza, Shaina, Caesar Saleh, Emrul Hasan, Franklin Ogidi, Maximus Powers, Veronica Chatrath, Marcelo Lotif, Roya Javadi, Anam Zahid, and Vahid Reza Khazaie. arXiv preprint arXiv:2412.17052 (2024).</p> </li> <li> <p>Fact or Fiction? Can LLMs Be Reliable Annotators for Political Truths?   Chatrath, Veronica, Marcelo Lotif, and Shaina Raza. arXiv preprint arXiv:2411.05775 (2024).</p> </li> </ul>"},{"location":"Publications/#media-coverage","title":"Media Coverage","text":"<ul> <li>New multimodal dataset will help in the development of ethical AI systems</li> <li>Neutralizing Bias in AI: Vector Institute\u2019s UNBIAS Framework Revolutionizes Ethical Text Analysis</li> <li>FairSense: Integrating Responsible AI and Sustainability</li> <li>YouTube Presentation: HumaniBench</li> </ul>"},{"location":"VLDBench/","title":"VLDBench: A Vision\u2013Language Benchmark for Multimodal Disinformation Detection","text":"<p>   \ud83c\udf10 Website \u00a0|\u00a0   \ud83d\udcc4 Paper \u00a0|\u00a0   \ud83d\udcca Dataset </p>"},{"location":"VLDBench/#overview","title":"\ud83e\udde0 Overview","text":"<p>As generative AI reshapes digital media, disinformation becomes harder to detect\u2014especially when combining text and images. VLDBench is the first large-scale human-verified multimodal disinformation benchmark, built to assess Language Models (LLMs) and Vision\u2013Language Models (VLMs) under real-world disinformation scenarios.</p> <p>Key Goals: - Evaluate trustworthiness, robustness, and governance alignment - Promote transparent AI evaluation in multimodal misinformation contexts</p>"},{"location":"VLDBench/#features","title":"\ud83d\udce6 Features","text":"<ul> <li>\ud83d\udcf0 62,000+ Image\u2013Article Pairs</li> <li>\ud83d\udc65 22 Domain Experts &amp; 500+ Hours of Verification</li> <li>\ud83e\uddea Binary Classification + Open-ended Reasoning</li> <li>\ud83e\udde0 Multimodal &amp; Unimodal Support (Text-only + Text+Image)</li> <li>\ud83c\udf10 Real News from 58 Global Outlets Across 13 Categories</li> <li>\ud83d\udd0d Perturbation-based Robustness Evaluation</li> </ul>"},{"location":"VLDBench/#evaluation-tasks-overview","title":"\ud83d\udcc2 Evaluation Tasks Overview","text":"Task Type Description Binary Classification Predict if a sample is Likely or Unlikely Disinformation (Text or Text+Image) Multimodal Reasoning Explain disinformation using visual + textual cues Robustness Testing Apply perturbations: noise, blur, text negation, cross-modal contradictions Instruction Fine-tuning Assess models\u2019 ability to learn from labeled examples Human Evaluation Rate correctness and reasoning clarity across LLMs and VLMs"},{"location":"VLDBench/#pipeline","title":"\ud83e\uddec Pipeline","text":"<p>VLDBench follows a five-stage process:</p> <ol> <li>Task Definition: Formalize disinformation detection</li> <li>Data Pipeline: Curate real-world articles + images from 58 sources</li> <li>Annotation: GPT-4o assisted with human review</li> <li>Expert Review: 22 domain experts verify each label</li> <li>Benchmarking: Evaluate LLMs and VLMs for accuracy, robustness, and governance compliance</li> </ol>"},{"location":"VLDBench/#benchmarked-models","title":"\ud83d\udcca Benchmarked Models","text":"<p>Evaluated Models: - 9 LLMs (e.g., Vicuna-7B, LLaMA-3.2-1B) - 10 VLMs (e.g., LLaVA-v1.6, GLM-4V-9B, LLaMA-3.2-11B-Vision)</p> <p>Key Result: \ud83d\udcc8 Multimodal models outperform text-only models by up to 15% in accuracy, but are more vulnerable to cross-modal attacks.</p> LLMs (Text-only) VLMs (Text+Image) Phi-3-mini-128k-instruct Phi-3-Vision-128k-Instruct Vicuna-7B-v1.5 LLaVA-v1.5-Vicuna7B Mistral-7B-Instruct-v0.3 LLaVA-v1.6-Mistral-7B Qwen2-7B-Instruct Qwen2-VL-7B-Instruct InternLM2-7B InternVL2-8B DeepSeek-V2-Lite-Chat Deepseek-VL2-small GLM-4-9B-chat GLM-4V-9B LLaMA-3.1-8B-Instruct LLaMA-3.2-11B-Vision LLaMA-3.2-1B-Instruct Deepseek Janus-Pro-7B \u2013 Pixtral"},{"location":"VLDBench/#key-insights","title":"\ud83d\udd11 Key Insights","text":"<p>\u2705 VLMs outperform LLMs on disinformation \u2705 Instruction-tuning improves accuracy by 5\u201310% \u2705 Combined attacks (text + image) reduce accuracy by up to 26% \u2705 Scalability improves performance, especially in larger VLMs \u2705 Human ratings confirm higher correctness and clarity in top models</p>"},{"location":"VLDBench/#citation","title":"\ud83d\udcda Citation","text":"<p>If you use VLDBench in your work, please cite:</p> <pre><code>@misc{raza2025vldbenchvisionlanguagemodels,\n  title={VLDBench: Vision Language Models Disinformation Detection Benchmark}, \n  author={Shaina Raza and Ashmal Vayani and Aditya Jain and Aravind Narayanan and Vahid Reza Khazaie and Syed Raza Bashir and Elham Dolatabadi and Gias Uddin and Christos Emmanouilidis and Rizwan Qureshi and Mubarak Shah},\n  year={2025},\n  eprint={2502.11361},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2502.11361}, \n}\n</code></pre>"},{"location":"VLDBench/#contact","title":"\ud83d\udcec Contact","text":"<p>For questions, collaborations, or dataset access requests, please contact the corresponding author at shaina.raza@vectorinstitute.ai.</p>"},{"location":"VLDBench/#vldbench-aims-to-foster-robust-fair-and-transparent-multimodal-ai","title":"\u26a1 VLDBench aims to foster robust, fair, and transparent multimodal AI.","text":"<p>We invite researchers, developers, and policymakers to explore, evaluate, and extend VLDBench. \ud83d\udd0d\ud83d\ude80</p>"},{"location":"dataset/","title":"Dataset Details","text":""},{"location":"dataset/#overview","title":"Overview","text":"<p>Our projects maintain a collection of datasets hosted on Hugging Face, focusing on human-centered AI, multimodal disinformation detection, and news media bias analysis. These datasets support reproducible research and benchmark development for responsible AI.</p>"},{"location":"dataset/#hugging-face-datasets-repositories","title":"Hugging Face Datasets &amp; Repositories","text":""},{"location":"dataset/#humanibench","title":"HumaniBench","text":"<ul> <li>Description: A large-scale benchmark with 32,000+ human-verified multilingual image-question pairs for evaluating fairness, ethics, reasoning, and empathy in Large Multimodal Models.</li> <li>Stats: 32.6k samples, 10+ languages, visual and textual annotations.</li> <li>Link: HumaniBench Dataset</li> </ul>"},{"location":"dataset/#vldbench","title":"VLDBench","text":"<ul> <li>Description: Benchmark for multimodal disinformation detection with 62,000+ real-world image-article pairs, human verified by domain experts.</li> <li>Stats: 62k+ samples, 58 news sources, multimodal labels.</li> <li>Link: VLDBench Dataset</li> </ul>"},{"location":"dataset/#newsmediabias-plus","title":"NewsMediaBias-Plus","text":"<ul> <li>Description: Comprehensive news articles dataset spanning multiple ideological sources, annotated for bias in text and images.</li> <li>Stats: 40.9k+ samples, multi-outlet, includes bias annotations and metadata.</li> <li>Link: NewsMediaBias-Plus Dataset</li> </ul>"},{"location":"dataset/#nmb-plus-bias-ner-bert","title":"NMB-Plus Bias NER BERT","text":"<ul> <li>Description: Named Entity Recognition model fine-tuned on NewsMediaBias-Plus for bias detection in entity mentions.</li> <li>Link: NER Model</li> </ul>"},{"location":"dataset/#llama32-multimodal-newsmedia-bias-detector","title":"Llama3.2 Multimodal Newsmedia Bias Detector","text":"<ul> <li>Description: Multimodal bias detection model leveraging Llama3.2 architecture to identify bias in combined text and images.</li> <li>Link: Multimodal Bias Detector</li> </ul>"},{"location":"dataset/#llama32-nlp-newsmedia-bias-detector","title":"Llama3.2 NLP Newsmedia Bias Detector","text":"<ul> <li>Description: NLP-based bias detector using Llama3.2, specialized for textual bias analysis in news media.</li> <li>Link: NLP Bias Detector</li> </ul>"},{"location":"dataset/#additional-repositories-and-models","title":"Additional Repositories and Models","text":"<ul> <li>NMB-Plus Clean Dataset \u2014 Cleaned news media bias dataset (31.3k samples).</li> <li>maximuspowers/nmbp-bert-full-articles \u2014 BERT-based text classification on full articles.</li> <li>maximuspowers/multimodal-bias-classifier \u2014 Multimodal bias classifier.</li> </ul>"},{"location":"dataset/#dataset-access-usage","title":"Dataset Access &amp; Usage","text":"<p>All datasets can be loaded via the Hugging Face <code>datasets</code> library. Example:</p> <pre><code>from datasets import load_dataset\n\n# HumaniBench datasets by task\nscene_understanding_ds = load_dataset(\"vector-institute/HumaniBench\", \"task1_Scene_Understanding\")\ninstance_identity_ds = load_dataset(\"vector-institute/HumaniBench\", \"task2_Instance_Identity\")\nmultiple_choice_vqa_ds = load_dataset(\"vector-institute/HumaniBench\", \"task3_Multiple_Choice_VQA\")\nmultilingual_open_ended_ds = load_dataset(\"vector-institute/HumaniBench\", \"task4_Multilingual_OpenEnded\")\nmultilingual_close_ended_ds = load_dataset(\"vector-institute/HumaniBench\", \"task4_Multilingual_CloseEnded\")\nvisual_grounding_ds = load_dataset(\"vector-institute/HumaniBench\", \"task5_Visual_Grounding\")\nempathetic_captioning_ds = load_dataset(\"vector-institute/HumaniBench\", \"task6_Empathetic_Captioning\")\nimage_resilience_ds = load_dataset(\"vector-institute/HumaniBench\", \"task7_Image_Resilience\")\n\n# Other datasets\nvldbench_ds = load_dataset(\"vector-institute/VLDBench\")\nnewsmediabias_plus_ds = load_dataset(\"vector-institute/newsmediabias-plus\")\nnmb_plus_clean_ds = load_dataset(\"vector-institute/nmb-plus-clean\")\nnmb_plus_named_entities_ds = load_dataset(\"vector-institute/NMB-Plus-Named-Entities\")\n\n</code></pre>"},{"location":"dataset/#news-sources-coverage","title":"News Sources &amp; Coverage","text":"<p>Our datasets cover a wide spectrum of news sources, including major US outlets, global media, and diverse political perspectives, ensuring comprehensive bias analysis capabilities.</p> <p>Refer to the detailed news sources list in the section below:</p> <ul> <li>Major U.S. News Outlets: CNN, Fox News, CBS News, ABC News, New York Times, Washington Post, USA Today, Wall Street Journal, AP News, Politico, New York Post, Forbes, Reuters, Bloomberg</li> <li>Global &amp; Alternative News Sources: BBC, Al Jazeera, PBS NewsHour, The Guardian, Newsmax, HuffPost, CNBC, C-SPAN, The Economist, Financial Times, Time, Newsweek, The Atlantic, The New Yorker, The Hill, ProPublica, Axios</li> <li>Conservative &amp; Progressive News Outlets: National Review, The Daily Beast, Daily Kos, Washington Examiner, The Federalist, OANN, Daily Caller, Breitbart</li> <li>Canadian News Sources: CBC, Toronto Sun, Global News, The Globe and Mail, National Post</li> </ul>"}]}