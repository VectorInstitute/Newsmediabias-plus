{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GPT-4o mini Agreement with Human Annotations on Article Labels\n",
    "This script evaluates the agreement of GPT-4o mini with human annotations on a dataset of articles labeled as 'Fake' or 'Real'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "executionInfo": {
     "elapsed": 4349,
     "status": "ok",
     "timestamp": 1723034526621,
     "user": {
      "displayName": "Shaina Raza",
      "userId": "10639723733218279727"
     },
     "user_tz": 240
    },
    "id": "IZhgwYD33Bh3",
    "outputId": "eba12b68-7773-4df8-f42a-4afd9d42edfd"
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pe6PH7qzhon"
   },
   "outputs": [],
   "source": [
    "key = 'sk-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392902,
     "status": "ok",
     "timestamp": 1723045369809,
     "user": {
      "displayName": "Shaina Raza",
      "userId": "10639723733218279727"
     },
     "user_tz": 240
    },
    "id": "ga-QPwZ6A_9f",
    "outputId": "661d188b-a131-4f2e-d3d3-b7c71f844d20"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = key\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'Summary_datasets/JUDGE_500_mistral.csv' # Replace with your actual file path for the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to query GPT-4o mini\n",
    "def query_model(article, label):\n",
    "    prompt = f\"\"\"\n",
    "                You are a fact-checking assistant.\n",
    "                Review the article below and determine if the label '{label}' accurately describes the article.\n",
    "                Respond with 'YES' if you agree with the label and 'NO' if you disagree with the label.\n",
    "\n",
    "                For 'Fake' labels:\n",
    "                - The article contains any misleading or false information, or is satirical.\n",
    "\n",
    "                For 'Real' labels:\n",
    "                - The article contains accurate and verifiable information.\n",
    "\n",
    "                Respond only with 'YES' or 'NO'.\n",
    "\n",
    "                Article:\n",
    "                {article}\n",
    "\n",
    "                Label:\n",
    "                {label}\n",
    "            \"\"\"\n",
    "# For 'Fake' labels:\n",
    "# - Presence of misleading information or disinformation\n",
    "\n",
    "\n",
    "# For 'Real' labels:\n",
    "# - Accuracy of information presented\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=3,  # Restricting the response length\n",
    "            temperature=0.0,  # Setting temperature to 0 for deterministic output\n",
    "            n=1,\n",
    "            stop=None\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluating annotations\n",
    "results = []\n",
    "for idx, row in data.iterrows():\n",
    "    agreement = query_model(row['text_content_summary'], row['label'])\n",
    "    results.append(agreement)\n",
    "    print(f\"Processed {idx+1}/{len(data)}\")\n",
    "\n",
    "# Add results to the dataframe and save it\n",
    "output_path = 'Summary_datasets/judge-results/4-mistral_with_agreement.csv' # Replace with your desired output path\n",
    "data['AGREE'] = results\n",
    "data.to_csv(output_path, index=False)\n",
    "print(\"File saved with agreement results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356068,
     "status": "ok",
     "timestamp": 1723054119278,
     "user": {
      "displayName": "Shaina Raza",
      "userId": "10639723733218279727"
     },
     "user_tz": 240
    },
    "id": "U4sh01lI76pu",
    "outputId": "16d3ba54-8bef-4017-ca3c-23e6838f68db"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = key\n",
    "# Load your dataset\n",
    "file_path = 'Summary_datasets/LLM Judge-majority_500.csv' # Replace with your actual file path for the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to query the model\n",
    "def query_model(article, model_name=\"gpt-4o-mini\"):\n",
    "    prompt = f\"\"\"\n",
    "                You are a helpful news fact-checking bot trained to assess the accuracy of information. Your task is to analyze the given article and determine whether it is 'Factually Correct' or 'Factually Incorrect'.\n",
    "\n",
    "                Fact-checking is the methodical process of verifying claims in public discourse or media reports. It is vital for countering misinformation and disinformation, thereby enhancing public knowledge and trust. Consider the following in your evaluation:\n",
    "\n",
    "                Misinformation: Incorrect or misleading information shared without intent to harm.\n",
    "                Disinformation: Information that is knowingly false, often prejudiced, and disseminated with the intent to mislead.\n",
    "\n",
    "                Your analysis should include:\n",
    "\n",
    "                Verification of key claims against multiple reliable sources.\n",
    "                Identification of logical fallacies or statements that may mislead readers.\n",
    "                Assessment of the context in which the information was presented, including the sourceâ€™s history and potential motivations.\n",
    "                Evaluation for any presence of hate speech, linguistic harm, or intent to spread prejudice.\n",
    "\n",
    "                Provide your assessment in the following format:\n",
    "\n",
    "                Classification: [Factually Correct/Factually Incorrect]\n",
    "                Explanation: Provide a concise, evidence-based explanation for your classification. Reference specific examples from the article and contradicting evidence from trusted sources, if applicable.\n",
    "\n",
    "                Ensure to remain objective, basing your assessment strictly on facts and evidence rather than personal opinions or biases.\n",
    "\n",
    "                Article to analyze:\n",
    "                {article}\n",
    "            \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=10,  # Adjusted for more expressive responses\n",
    "            temperature=0.0\n",
    "        )\n",
    "        # Extracting the message content correctly\n",
    "        message_content = response.choices[0].message['content'].strip()\n",
    "        # Determine the classification based on the model's response\n",
    "        if \"Factually Correct\" in message_content:\n",
    "            classification = \"Real\"\n",
    "        elif \"Factually Incorrect\" in message_content:\n",
    "            classification = \"Fake\"\n",
    "        else:\n",
    "            classification = \"Unknown\"\n",
    "        return classification\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluating annotations\n",
    "results = []\n",
    "for idx, row in data.iterrows():\n",
    "    classification = query_model(row['text_content_summary'])\n",
    "    results.append(classification)\n",
    "    print(f\"Processed {idx+1}/{len(data)}\")\n",
    "\n",
    "# Add results to the dataframe and save it\n",
    "output_path = 'Summary_datasets/judge-results/4-gpt4o-labels.csv' # Replace with your desired output path\n",
    "data['Classification'] = results\n",
    "data.to_csv(output_path, index=False)\n",
    "print(\"File saved with agreement results.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCUMcbCFgJKraTLuMYDywN",
   "mount_file_id": "123KfVYEH-mKQV_TmJI7pUKzXyrlCgOXl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
