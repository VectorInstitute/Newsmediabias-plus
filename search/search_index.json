{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"News Media Bias Plus Project","text":""},{"location":"#our-mission","title":"Our Mission","text":"<p>The News Media Bias Plus Project is a leading initiative in the field of responsible AI, dedicated to advancing the understanding of media bias and disinformation through the lens of artificial intelligence. We focus on the critical intersection between AI safety and media integrity, with the goal of promoting a more balanced and transparent information ecosystem. Our key areas of interest include:</p> <ul> <li>Bias Detection: Identifying and addressing biases in both AI systems and media content.</li> <li>Disinformation Challenges: Combatting misinformation and its societal impact.</li> <li>Ethical AI: Advocating for the responsible use of AI in media production and journalism.</li> </ul>"},{"location":"#our-framework","title":"Our Framework","text":"<p>By leveraging cutting-edge AI techniques, we analyze diverse media formats\u2014including news articles, documents, and images\u2014to detect, categorize, and mitigate various forms of media bias. Our comprehensive system assesses:</p> <ul> <li>Topic Coverage and Framing: How media outlets present and prioritize different subjects.</li> <li>Ideological Leanings and Sentiment: The underlying tone and political inclinations in media.</li> <li>Language Patterns and Tone: Examination of stylistic choices that influence perception.</li> <li>Source Credibility and Transparency: Evaluation of the reliability and openness of information sources.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Bias Analysis: Comparative analysis of media coverage across multiple sources to highlight differences in bias and framing.</li> <li>AI Safety Metrics: Monitoring and assessing the role of AI in content generation and bias detection, ensuring its responsible deployment.</li> <li>Disinformation Alerts: Identifying potential AI-generated disinformation, including deepfakes and fabricated news.</li> <li>Interactive Visualizations: Engaging tools that allow users to explore media bias trends and the growing influence of AI in journalism.</li> </ul>"},{"location":"#why-it-matters","title":"Why It Matters","text":"<p>In an era where artificial intelligence is increasingly shaping the media landscape, understanding its role in media bias and disinformation is essential for:</p> <ul> <li>Promoting Media Literacy: Empowering individuals to critically evaluate news content and sources.</li> <li>Ensuring Responsible AI Development: Fostering the ethical use of AI in journalism and news production.</li> <li>Building Trust: Strengthening trust in AI systems and media institutions by improving transparency and accountability.</li> </ul>"},{"location":"#dataset-access","title":"Dataset Access","text":""},{"location":"#download-from-huggingface","title":"Download from Huggingface","text":"<p>NewsMediaBias-Plus Dataset on Hugging Face</p>"},{"location":"#download-parquet-and-images","title":"Download Parquet and Images","text":"<p>Zenodo Record</p>"},{"location":"#get-involved-contact-us","title":"Get Involved &amp; Contact Us","text":"<p>We invite researchers, developers, and the broader public to contribute to our efforts in combating media bias and disinformation. You can support our project by:</p> <ul> <li>Collaborating on bias detection and disinformation research.</li> <li>Requesting Data Access or providing feedback by completing the form below:</li> </ul> Loading\u2026 <p>Contact Information: Vector Institute for Artificial Intelligence Schwartz Reisman Innovation Campus 108 College St., Suite W1140 Toronto, ON M5G 0C6  </p> <p>Email: shaina.raza@vectorinstitute.ai</p>"},{"location":"About-us/","title":"Team","text":""},{"location":"About-us/#project-lead","title":"Project Lead","text":"Dr. Shaina Raza Applied Machine Learning Scientist, Responsible AI"},{"location":"About-us/#contributors","title":"Contributors","text":"Marcelo Lotif Senior Software Developer &amp; ML Engineer, Vector Institute    Emrul Hasan Ph.D. Candidate, Toronto Metropolitan University, Vector Institute    Veronica Chatrath Associate Technical Program Manager, Vector Institute    Franklin Ogidi Associate Machine Learning Specialist, Vector Institute    Caesar Saleh Intern, University of Toronto, Vector Institute    Roya Javadi Machine Learning Software Developer, Vector Institute    Sina Salimian Research Assistant, University of Calgary    Maximus Powers Ethical Spectacle Research    Mark Coatsworth Vector Institute"},{"location":"About-us/#advisors","title":"Advisors","text":"Dr. Arash Afkanpour  Vector Institute    Dr. Gias Uddin Professor, York University    Dr. Aditya Jain Applied Research Scientist, Meta"},{"location":"About-us/#acknowledgments","title":"Acknowledgments","text":"<p>We extend our sincere thanks to Michael Joseph, Manoj Athreya, Sherry, Sara Kodeiri, Roya Javadi, Fatemeh Tavakoli, Nan Ajmain, Wu Rupert, and the entire team for their valuable assistance in reviewing the data.</p>"},{"location":"Annotation/","title":"Annotation Framework","text":""},{"location":"Annotation/#1-annotation-guidelines-and-procedure","title":"1. Annotation Guidelines and Procedure","text":"<p>This framework outlines a structured approach for annotating news articles, incorporating both text and images. The process begins with human annotators labeling a carefully selected subset of the data. Once this subset is annotated, Large Language Models (LLMs) take over to expand these labels across the entire dataset. By aligning annotations with corresponding text and images, the result is a Silver Standard Dataset.</p>"},{"location":"Annotation/#2-quality-control","title":"2. Quality Control","text":"<p>To ensure the reliability and consistency of annotations, multiple quality control mechanisms are in place. Cohen's Kappa is employed to measure inter-annotator agreement, highlighting areas that may require further clarification. In addition to automated checks, human reviewers manually evaluate a portion of the annotations. This dual approach maintains the quality and accuracy of the labeled data.</p>"},{"location":"Annotation/#3-evaluation-pipeline","title":"3. Evaluation Pipeline","text":"<p>The evaluation process is designed to convert the Silver Standard Dataset into a Gold Standard Dataset. Initially, an LLM-based jury provides judgments on the quality of the annotations. These judgments are then reviewed by human experts, who validate, refine, or discard annotations as necessary. This collaborative effort between machines and human reviewers ensures a high-quality final dataset.</p>"},{"location":"Annotation/#4-system-training-and-testing","title":"4. System Training and Testing","text":"<p>Once the Gold Standard Dataset is established, it serves as the foundation for training and testing models in multi-modal bias detection. This process ensures the model\u2019s performance remains robust across various data types, including both text and images.</p>"},{"location":"Benchmark/","title":"Benchmarking Annotation Framework","text":""},{"location":"Benchmark/#overview","title":"Overview","text":"<p>This page presents the performance benchmarking of Small Language Models (SLMs) and Large Language Models (LLMs) within our annotation framework. The objective is to evaluate how these models perform in tasks involving text and multimodal data (text + image). For this benchmarking, SLMs are defined as models with fewer parameters, typically below 15 million, such as BERT and GPT-2. In contrast, LLMs\u2014including models like Llama3, Mistral, Gemma, and Phi\u2014possess hundreds of millions to billions of parameters. This scale difference highlights the trade-off between efficiency and complexity when handling various tasks and datasets.</p>"},{"location":"Benchmark/#benchmarking-results-text-based-models","title":"Benchmarking Results: Text-Based Models","text":"<p>The following table summarizes the performance of state-of-the-art text-based models. \"FT\" stands for Fine Tuning, \"IFT\" for Instruction Fine Tuning, and models have been evaluated across different metrics such as Precision, Recall, F1-Score, and Accuracy.</p> Model Architecture Precision Recall F1-Score Accuracy BERT-base-uncased (FT) Encoder-only 0.8887 0.887 0.8846 0.8870 DistilBERT (FT) Encoder-only 0.8665 0.8554 0.8487 0.8661 RoBERTa-base (FT) Encoder-only 0.894 0.894 0.8927 0.8940 GPT2 (FT) Decoder-only 0.8762 0.8751 0.8727 0.8751 BART (FT) Encoder-decoder 0.8762 0.876 0.874 0.8760 Llama 3.1-8B-instruct (0-shot) Decoder-only 0.828 0.689 0.7525 0.7200 Llama 3.1-8B-instruct (5-shots) Decoder-only 0.840 0.770 0.8034 0.7905 Llama 3.1-8B-instruct (IFT) Decoder-only 0.8019 0.8019 0.8418 0.8180 Llama 3.1-8B (FT) Decoder-only 0.800 0.800 0.790 0.8320 Llama 3.2-3B-instruct (0-shot) Decoder-only 0.7386 0.4622 0.5012 0.3897 Llama 3.2-3B-instruct (5-shots) Decoder-only 0.7989 0.3840 0.3763 0.4622 Llama 3.2-3B-instruct (IFT) Decoder-only 0.839 0.7984 0.8182 0.8084 Llama 3.2-3B-sequence classifier (FT) Decoder-only 0.840 0.850 0.840 0.8200 Mistral-v0.3-instruct (0-shot) Decoder-only 0.8153 0.5250 0.6380 0.6990 Mistral-v0.3-instruct (5-shots) Decoder-only 0.8319 0.8134 0.8225 0.8230 Mistral-v0.3-instruct (IFT) Decoder-only 0.889 0.924 0.9062 0.8680 Mistral-v0.3 (FT) Decoder-only 0.820 0.820 0.820 0.8050 qwen2.5-7B (0-shot) Decoder-only 0.8576 0.8576 0.8576 0.8576 qwen2.5-7B (5-shots) Decoder-only 0.8660 0.8790 0.8720 0.8900"},{"location":"Benchmark/#benchmarking-results-multimodal-models-text-image","title":"Benchmarking Results: Multimodal Models (Text + Image)","text":"<p>The following table highlights the performance of multimodal models, which process both text and image inputs. These models were evaluated on weighted averages for Precision, Recall, and F1.</p> Model FT/IFT Architecture (Text-Image) Precision Recall F1 SpotFake (XLNET + VGG-19) FT Encoder-Encoder 0.7415 0.6790 0.7040 BERT + ResNet-34 FT Encoder-Encoder 0.8311 0.6277 0.6745 FND-CLIP (BERT and CLIP/ResNET + CLIP) FT Encoder-Encoder 0.6935 0.7151 0.7035 InstructBlipV (Text + Images) FT Encoder-Encoder - - - Distill-bert/CLIP FT Encoder-Encoder 0.5472 0.4047 0.4652 google/paligemma-3b-pt-224 IFT Decoder-Encoder - - - microsoft/Phi-3-vision-128k-instruct 0-shot Decoder-Encoder - - - microsoft/Phi-3-vision-128k-instruct 5-shots Decoder-Encoder - - - Pixtral-12B-2409 0-shot Decoder-Encoder - - - Pixtral-12B-2409 2-shots Decoder-Encoder - - - LLaVA-1.6 0-shot Decoder-Encoder - - - Llama-3.2-11B-Vision-Instruct 0-shot Decoder-Encoder - - - meta-llama/Llama-Guard-3-11B-Vision FT Decoder-Encoder - - - <p>This benchmarking offers an insightful overview of how various models, ranging from smaller to large-scale, perform in distinct environments and tasks. The text-based and multimodal benchmarks reflect the strength of these models in handling the complexities of both textual data and combined text-image inputs, providing a useful reference for selecting the appropriate model based on the task requirements.</p>"},{"location":"Publications/","title":"Publications","text":""},{"location":"Publications/#journal-articles","title":"Journal Articles","text":""},{"location":"Publications/#conference-papers","title":"Conference Papers","text":""},{"location":"Publications/#media-coverage","title":"Media Coverage","text":"<p>List instances of media coverage, including articles, interviews, and mentions in popular press. Provide links and a brief description of each piece.</p>"},{"location":"dataset.md/","title":"Dataset Details","text":""},{"location":"dataset.md/#news-sources","title":"News Sources","text":"<p>Our dataset includes articles from a broad range of reputable news organizations across the political and ideological spectrum, ensuring a comprehensive view of media bias:</p> <ul> <li>Major U.S. News Outlets: CNN, Fox News, CBS News, ABC News, New York Times, Washington Post, USA Today, Wall Street Journal, AP News, Politico, New York Post, Forbes, Reuters, Bloomberg</li> <li>Global &amp; Alternative News Sources: BBC, Al Jazeera, PBS NewsHour, The Guardian, Newsmax, HuffPost, CNBC, C-SPAN, The Economist, Financial Times, Time, Newsweek, The Atlantic, The New Yorker, The Hill, ProPublica, Axios</li> <li>Conservative &amp; Progressive News Outlets: National Review, The Daily Beast, Daily Kos, Washington Examiner, The Federalist, OANN, Daily Caller, Breitbart</li> <li>Canadian News Sources: CBC, Toronto Sun, Global News, The Globe and Mail, National Post</li> </ul>"},{"location":"dataset.md/#date-range","title":"Date Range","text":"<p>The dataset spans from May 6, 2023 to September 6, 2024. Articles were collected using Python scripts incorporating feedparser, newspaper3k, and selenium to enable keyword-based searches, custom date ranges, deduplication of articles, and image downloads.</p>"},{"location":"dataset.md/#key-features","title":"Key Features","text":"<ul> <li>Multi-source Scraping: Collects articles from diverse media outlets.</li> <li>Keyword-based Search: Allows focused scraping on specific topics or terms.</li> <li>Comprehensive Data Collection: Captures both text and images from articles.</li> <li>Deduplication: Ensures only unique articles are included in the dataset.</li> <li>Structured Output: Outputs data in CSV format for easy analysis and processing.</li> </ul>"},{"location":"dataset.md/#dataset-schema","title":"Dataset Schema","text":"<p>The dataset schema is designed for bias assessment and structured analysis of media content, including both textual and image data:</p> <pre><code>-- news_article_analysis (\n    unique_id VARCHAR(255) PRIMARY KEY,\n    outlet VARCHAR(255),\n    headline TEXT,\n    article_text TEXT,\n    image_description TEXT,\n    image BLOB,  \n    date_published VARCHAR(255),\n    source_url VARCHAR(255),\n    canonical_link VARCHAR(255),\n    new_categories TEXT,\n    news_categories_confidence_scores TEXT,\n    text_label VARCHAR(255),\n    multimodal_label VARCHAR(255)\n)\n</code></pre>"},{"location":"dataset.md/#access","title":"Access","text":""},{"location":"dataset.md/#dataset-access","title":"Dataset Access","text":"<p>You can access the NewsMediaBias-Plus dataset via the following link:</p> <p>NewsMediaBias-Plus Dataset on Hugging Face</p>"},{"location":"dataset.md/#usage","title":"Usage","text":"<p>To load the full dataset into your Python environment, use the following code:</p> <pre><code>from datasets import load_dataset\n\nds = load_dataset(\"vector-institute/newsmediabias-plus\")\nprint(ds)  # Displays structure and splits\nprint(ds['train'][0])  # Access the first element of the train split\nprint(ds['train'][:5])  # Access the first five elements of the train split\n</code></pre> <p>The dataset is also available for download in Parquet format, along with the corresponding images, via Zenodo:</p>"},{"location":"dataset.md/#download-parquet-and-images","title":"Download Parquet and Images","text":"<p>Zenodo Record</p>"},{"location":"dataset.md/#sample-data","title":"Sample Data","text":""},{"location":"dataset.md/#article-1-sex-trafficking-victim-says-sen-katie-britt-telling-her-story-during-sotu-rebuttal-is-not-fair-cnn","title":"Article 1: Sex trafficking victim says Sen. Katie Britt telling her story during SOTU rebuttal is 'not fair' - CNN","text":"<ul> <li>Unique ID: 1098444910</li> <li>Title: Sex trafficking victim says Sen. Katie Britt telling her story during SOTU rebuttal is 'not fair' - CNN</li> <li>Text: CNN \u2014 The woman whose story Alabama Sen. Katie Britt appeared to have shared in the Republican response to the State of the Union as an example of President Joe Biden\u2019s failed immigration policies told CNN she was trafficked before Biden\u2019s presidency and said legislators lack empathy when using the issue of human trafficking for political purposes.  <p>\"I hardly ever cooperate with politicians, because it seems to me that they only want an image. They only want a photo \u2014 and that to me is not fair,\" Karla Jacinto told CNN on Sunday.</p> </li> <li>Outlet: CNN</li> <li>Source URL: CNN</li> <li>Topics: 5_bipartisan, border, border deal, border policy, border wall</li> <li>Date Published: 2024-03-10</li> <li>Image Description:  <p>The image shows a person standing at a podium with a microphone, appearing to be giving a speech or presentation. The individual is wearing a pink blazer with a white shirt underneath. The background is indistinct but suggests an indoor setting with a wooden structure, possibly a room with a high ceiling. There are no visible logos, text, or other identifying features that provide context to the event or the person's identity.</p> </li> <li>Text Label: Unlikely</li> <li>Text Bias Analysis: <p>\"failed immigration policies\", \"lack of empathy\", \"despicable\", \"almost entirely preventable\"</p> </li> <li>Image Label: Unlikely</li> <li>Image Analysis: <p>The image alone does not provide enough context to analyze potential biases. The choice of the image could be influenced by the event's significance, the person's role, or the visual impact of the pink blazer. Without additional information, it is not possible to determine if the image is biased or Unbiased. The image does not appear to evoke strong emotions as it is a straightforward depiction of a person at a podium. There are no clear indications of stereotypes or oversimplification of complex issues in the image.</p> </li> </ul>"},{"location":"dataset.md/#article-2-las-graffiti-tagged-skyscraper-a-work-of-art-and-symbol-of-citys-wider-failings-the-guardian-us","title":"Article 2: LA\u2019s graffiti-tagged skyscraper: a work of art \u2013 and symbol of city\u2019s wider failings - The Guardian US","text":"<ul> <li>Unique ID: 1148232027</li> <li>Title: LA\u2019s graffiti-tagged skyscraper: a work of art \u2013 and symbol of city\u2019s wider failings - The Guardian US</li> <li>Text:  <p>An asparagus patch is how the architect Charles Moore described the lackluster skyline of downtown Los Angeles in the 1980s. \"The tallest stalk and the shortest stalk are just alike, except that the tallest has shot farther out of the ground.\" This sprawling city of bungalows has never been known for the quality of its high-rise buildings, and not much has changed since Moore\u2019s day. A 1950s ordinance dictating that every tower must have a flat roof was rescinded in 2014, spawning a handful of clumsy quiffs and crowns atop a fresh crop of swollen glass slabs. It only added further evidence to the notion that architects in this seismic city are probably better suited to staying on the ground.</p> </li> <li>Outlet: The Guardian US</li> <li>Source URL: The Guardian US</li> <li>Topics: affordable housing, public housing, homeowners, housing crisis</li> <li>Date Published: 2024-03-17</li> <li>Image Description:  <p>The image shows a tall, multi-story building with numerous windows. The building is covered in various graffiti tags and symbols, with words like 'READY', 'SHAKA', 'RAKM', 'TOOL', 'TOLT', 'KERZ', 'SMK', 'DZER', 'MSK', and 'OBER' prominently displayed. The building is situated in an urban environment with other structures visible in the background. The sky is clear, suggesting it might be daytime. The image is taken from a high angle, looking down on the building.</p> </li> <li>Text Label: Likely</li> <li>Text Bias Analysis: <p>\"mind-numbingly generic glass boxes\", \"abandoned\", \"doing nothing\", \"if they ain\u2019t gon finish the job\", \"This building has needed love for years\", \"the streets of LA are happy to make something out of it\", \"the developer had ceased paying\"</p> </li> <li>Image Label: Likely</li> <li>Image Analysis: <p>The image and accompanying headline from The Guardian US suggest a critical perspective on the state of urban development and the impact of graffiti on architecture. The choice of this image may be intended to highlight the issue of urban decay and the lack of maintenance in certain areas. The graffiti tags could be seen as a form of artistic expression, but within the context of the headline, they are likely to be interpreted as a symbol of the city's wider failings. The image does not provide a balanced view, as it focuses on the negative aspects of the building's appearance. The framing of the image, with the building as the central focus and the surrounding environment in the background, may lead viewers to associate the building's condition with the overall state of the city.</p> </li> </ul>"},{"location":"goverance.md/","title":"Goverance.md","text":"<p>This is goverance thing to be done by Safi</p> <p>what are frameworks in eu, usa, canada which consider bias n disinformaton (list)</p> <p>how we are doing this</p>"}]}