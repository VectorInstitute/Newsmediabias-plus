{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"News Media Bias Plus Project","text":""},{"location":"#our-mission","title":"Our Mission","text":"<p>The News Media Bias Plus Project is dedicated to promoting responsible AI development and addressing critical challenges in artificial intelligence, with a special focus on media bias and disinformation. We explore the intersection of AI safety and media integrity, focusing on:</p> <ul> <li>Bias Detection: Uncovering and mitigating biases in AI systems and media content</li> <li>Disinformation Challenges: Addressing misinformation and its societal impact </li> <li>Ethical AI: Promoting responsible use of AI in news reporting and production</li> </ul>"},{"location":"#our-framework","title":"Our Framework","text":"<p>Using state-of-the-art AI methods, we analyze news articles, documents and images to detect and categorize different types of media bias. Our system examines:</p> <ul> <li>Topic coverage and framing</li> <li>Ideological leanings and sentiment</li> <li>Language patterns and tone</li> <li>Source credibility and transparency</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Bias Analysis: Compare coverage of specific topics across multiple news sources</li> <li>AI Safety Metrics: Track the use of AI in content analysis and its impact on bias</li> <li>Disinformation Alerts: Detect and flag potential AI-generated fake news or deepfakes</li> <li>Interactive Visualizations: Explore media bias trends and AI influence in journalism</li> </ul>"},{"location":"#why-it-matters","title":"Why It Matters","text":"<p>Understanding the role of AI in media bias and disinformation is crucial for:</p> <ul> <li>Promoting media literacy in the age of AI</li> <li>Ensuring responsible AI development in journalism</li> <li>Fostering trust in both AI systems and media institutions</li> </ul>"},{"location":"#get-involved-contact-us","title":"Get Involved &amp; Contact Us","text":"<p>We welcome your contributions, questions, and feedback. Here's how you can engage with our project:</p> <ul> <li>Contribute to our bias and disinformation detection efforts</li> <li>For data access requests or other inquiries, please fill out this form:</li> </ul> Loading\u2026 <p>Address: Vector Institute for Artificial Intelligence Schwartz Reisman Innovation Campus 108 College St., Suite W1140 Toronto, ON M5G 0C6  </p> <p>Email: shaina.raza@vectorinstitute.ai</p>"},{"location":"About-us/","title":"Team","text":""},{"location":"About-us/#project-lead","title":"Project Lead","text":"Dr. Shaina Raza: Applied Machine Learning Scientist, Responsible AI"},{"location":"About-us/#contributors","title":"Contributors","text":"Marcelo Lotif: Senior Software Developer &amp; ML Engineer , Vector Institute    Caesar Saleh: Undergraduate Researcher, University of Toronto,  Vector Institute    Emrul Hasan: Ph.D. Candidate, Toronto Metropolitan University, Vector Institute    Veronica Chatrath: Associate Technical Program Manager , Vector Institute    Franklin Ogidi: Associate Machine Learning Specialist , Vector Institute    Roya Javadi: Machine Learning Software Engineer, Vector Institute    Sina Salimian: Research Assistant, Univerity of Calgary.    Aditya Jain: Computational Scientist, Meta    Dr. Gias Uddin: Professor, York University"},{"location":"Annotation/","title":"Annotation Framework","text":""},{"location":"Annotation/#1-annotation-guidelines-and-procedure","title":"1. Annotation Guidelines and Procedure","text":"<p>This framework outlines a structured approach for annotating news articles, incorporating both text and images. The process begins with human annotators labeling a carefully selected subset of the data. Once this subset is annotated, Large Language Models (LLMs) take over to expand these labels across the entire dataset. By aligning annotations with corresponding text and images, the result is a Silver Standard Dataset.</p>"},{"location":"Annotation/#2-quality-control","title":"2. Quality Control","text":"<p>To ensure the reliability and consistency of annotations, multiple quality control mechanisms are in place. Cohen's Kappa is employed to measure inter-annotator agreement, highlighting areas that may require further clarification. In addition to automated checks, human reviewers manually evaluate a portion of the annotations. This dual approach maintains the quality and accuracy of the labeled data.</p>"},{"location":"Annotation/#3-evaluation-pipeline","title":"3. Evaluation Pipeline","text":"<p>The evaluation process is designed to convert the Silver Standard Dataset into a Gold Standard Dataset. Initially, an LLM-based jury provides judgments on the quality of the annotations. These judgments are then reviewed by human experts, who validate, refine, or discard annotations as necessary. This collaborative effort between machines and human reviewers ensures a high-quality final dataset.</p>"},{"location":"Annotation/#4-system-training-and-testing","title":"4. System Training and Testing","text":"<p>Once the Gold Standard Dataset is established, it serves as the foundation for training and testing models in multi-modal bias detection. This process ensures the model\u2019s performance remains robust across various data types, including both text and images.</p>"},{"location":"Benchmark/","title":"Benchmarking for Annotation Framework","text":""},{"location":"Benchmark/#purpose","title":"Purpose","text":"<p>The purpose of this benchmarking page is to evaluate the performance of Small Language Models (SLMs) and Large Language Models (LLMs) in our annotation framework. In this context, we refer to SLMs as those with fewer parameters, typically less than 15 million, such as BERT and GPT-2. LLMs, like Llama3, Mistral, Gemma, Phi, have significantly more parameters, often in the hundreds of millions to billions. This relative difference in scale allows us to compare the efficiency to handle more complex tasks and datasets, while SLMs are more efficient for simpler tasks or environments with limited resources.</p>"},{"location":"Benchmark/#benchmarking-on-texts","title":"Benchmarking on Texts","text":""},{"location":"Benchmark/#small-language-models-slms","title":"Small Language Models (SLMs)","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) BERT-base-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BERT-large-uncased Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A DistilBERT Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A RoBERTa-base Fine-tuning Encoder-only Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A GPT2 Fine-tuning Decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A BART Fine-tuning Encoder-decoder Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A"},{"location":"Benchmark/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"Benchmark/#llama-models","title":"Llama Models","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Llama 3.1-8B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.1-8B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B-Instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-1B Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-3B-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias/Likely (0), Real/Unbias/Unlikely (1) N/A Llama 3.2-8B-sequence classifier Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Llama 3 (70B) N/A N/A N/A 1900"},{"location":"Benchmark/#other-llms","title":"Other LLMs","text":"Model Training Method Architecture Classes Carbon Emissions (tCO\u2082e) Mistral-v0.3-instruct 0-shot, 5-shot, IFT Decoder-only autoregressive CausalLM Fake/Bias, Real/Unbias N/A Mistral-v0.3 Fine-tuning Decoder-only autoregressive sequence classification Fake/Bias, Real/Unbias N/A Mistral-large-instruct (IFT) IFT N/A Fake/Bias, Real/Unbias N/A Gemma-2-9b-Instruct 0-shot, 5-shot, IFT Decoder-only, Causal LM Fake/Bias, Real/Unbias N/A Gemma-2-9b Fine-tuning Decoder-only, sequence classification Fake/Bias, Real/Unbias N/A"},{"location":"Benchmark/#benchmarking-on-multimodality","title":"Benchmarking on Multimodality","text":""},{"location":"Benchmark/#small-language-models-slms_1","title":"Small Language Models (SLMs)","text":"Model Training Method Architecture (text-image) BERT + ResNet-34 Fine-tuning Encoder-Encoder SAFE (Text-CNN + Image2Sentence) Fine-tuning Encoder-Encoder SpotFake (XLNET + VGG-19) Fine-tuning Encoder-encoder MCAN (BERT + VGG-19/CNN) Fine-tuning Encoder-encoder FND-CLIP (BERT/ResNet + CLIP) Fine-tuning Encoder-encoder InstructBlipV Fine-tuning Encoder-encoder DistilBERT + CLIP Fine-tuning Encoder-encoder"},{"location":"Benchmark/#large-language-models-llms_1","title":"Large Language Models (LLMs)","text":"Model Training Method Architecture (text-image) google/paligemma-3b-pt-224 Instruction fine-tuning Decoder-encoder microsoft/Phi-3-vision-128k-instruct 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder Pixtral-12B-2409 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder LLaVA-1.6 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder Llama-3.2-11B-Vision-Instruct 0-shot, 5-shot, Instruction fine-tuning Decoder-encoder meta-llama/Llama-3.2-11B-Vision Fine-tuning Decoder-encoder meta-llama/Llama-Guard-3-11B-Vision Inference Decoder-encoder"},{"location":"Publications/","title":"Publications","text":""},{"location":"Publications/#journal-articles","title":"Journal Articles","text":""},{"location":"Publications/#conference-papers","title":"Conference Papers","text":""},{"location":"Publications/#media-coverage","title":"Media Coverage","text":"<p>List instances of media coverage, including articles, interviews, and mentions in popular press. Provide links and a brief description of each piece.</p>"},{"location":"dataset.md/","title":"Dataset Details","text":""},{"location":"dataset.md/#news-sources","title":"News Sources","text":"<ul> <li>CNN, Fox News, CBS News, ABC News, New York Times</li> <li>Washington Post, BBC, USA Today, Wall Street Journal</li> <li>AP News, Politico, New York Post, Forbes, Reuters</li> <li>Bloomberg, Al Jazeera, PBS NewsHour, The Guardian</li> <li>Newsmax, HuffPost, CNBC, C-SPAN, The Economist</li> <li>Financial Times, Time, Newsweek, The Atlantic</li> <li>The New Yorker, The Hill, ProPublica, Axios</li> <li>National Review, The Daily Beast, Daily Kos</li> <li>Washington Examiner, The Federalist, OANN</li> <li>Daily Caller, Breitbart, CBC, Toronto Sun</li> <li>Global News, The Globe and Mail, National Post</li> </ul>"},{"location":"dataset.md/#date-range","title":"Date Range","text":"<ul> <li>Date range : 2023-05-06 till 2024-09-06 Python script using feedparser, newspaper3k, and selenium to scrape articles from multiple news sources, supporting keyword searches, custom date ranges, and outputting data to CSV files, with features for deduplication and image downloading.</li> </ul>"},{"location":"dataset.md/#key-features","title":"Key Features","text":"<ul> <li>Scrapes articles from multiple news sources</li> <li>Supports keyword-based searches</li> <li>Downloads and stores article content and images</li> <li>Deduplicates articles based on unique identifiers</li> <li>Outputs data to CSV files</li> </ul>"},{"location":"dataset.md/#dataset-schema","title":"Dataset Schema","text":"<pre><code>-- news_article_analysis (\nunique_id VARCHAR(255) PRIMARY KEY,\n\n-- Bias assessment\ntext_label VARCHAR(10) CHECK (text_label IN ('Likely', 'Unlikely')),\nimage_label VARCHAR(10) CHECK (image_label IN ('Likely', 'Unlikely')),\ntext_label_reason TEXT,\nimage_label_reason TEXT,\nnews_category VARCHAR(255),\n\n-- Article content\ntitle VARCHAR(255),\ncanonical_link VARCHAR(255),\nfirst_paragraph TEXT,\noutlet VARCHAR(100),\nsource_url VARCHAR(255),\ntopics TEXT,\ntext_content TEXT,\ndate_published TIMESTAMP,\n\n-- Image details\nimg_description TEXT,\nimage_filename VARCHAR(255),\n\n-- Topic modeling\nbertopics TEXT\n</code></pre> <p>)</p>"},{"location":"dataset.md/#access","title":"Access","text":"<p>Here's the information formatted in MkDocs style:</p>"},{"location":"dataset.md/#dataset-access","title":"Dataset Access","text":""},{"location":"dataset.md/#train","title":"Train","text":"<pre><code>https://example.com/datasets/train_data.csv\n</code></pre>"},{"location":"dataset.md/#val","title":"Val","text":"<pre><code>https://example.com/datasets/train_data.csv```\n\n### Test\n</code></pre> <p>https://example.com/datasets/train_data.csv```</p> <p>This format provides clear and organized access to the dataset links for train, validation, and test sets. Users can easily copy the URLs for each dataset split.</p>"},{"location":"dataset.md/#sample-data","title":"Sample Data","text":"<p>Certainly! I'll create a sample dataset with 3 entries based on the schema provided. Here's how you can present it in MkDocs format:</p>"},{"location":"dataset.md/#sample-data_1","title":"Sample Data","text":""},{"location":"dataset.md/#article-1-sex-trafficking-victim-says-sen-katie-britt-telling-her-story-during-sotu-rebuttal-is-not-fair-cnn","title":"Article 1: Sex trafficking victim says Sen. Katie Britt telling her story during SOTU rebuttal is 'not fair' - CNN","text":"<ul> <li>Unique ID: 1098444910</li> <li>Title: Sex trafficking victim says Sen. Katie Britt telling her story during SOTU rebuttal is 'not fair' - CNN</li> <li>Text: CNN \u2014 The woman whose story Alabama Sen. Katie Britt appeared to have shared in the Republican response to the State of the Union as an example of President Joe Biden\u2019s failed immigration policies told CNN she was trafficked before Biden\u2019s presidency and said legislators lack empathy when using the issue of human trafficking for political purposes.  <p>\"I hardly ever cooperate with politicians, because it seems to me that they only want an image. They only want a photo \u2014 and that to me is not fair,\" Karla Jacinto told CNN on Sunday.</p> </li> <li>Outlet: CNN</li> <li>Source URL: CNN</li> <li>Topics: 5_bipartisan, border, border deal, border policy, border wall</li> <li>Date Published: 2024-03-10</li> <li>Image Description:  <p>The image shows a person standing at a podium with a microphone, appearing to be giving a speech or presentation. The individual is wearing a pink blazer with a white shirt underneath. The background is indistinct but suggests an indoor setting with a wooden structure, possibly a room with a high ceiling. There are no visible logos, text, or other identifying features that provide context to the event or the person's identity.</p> </li> <li>Text Label: Unlikely</li> <li>Text Bias Analysis: <p>\"failed immigration policies\", \"lack of empathy\", \"despicable\", \"almost entirely preventable\"</p> </li> <li>Image Label: Unlikely</li> <li>Image Analysis: <p>The image alone does not provide enough context to analyze potential biases. The choice of the image could be influenced by the event's significance, the person's role, or the visual impact of the pink blazer. Without additional information, it is not possible to determine if the image is biased or Unbiased. The image does not appear to evoke strong emotions as it is a straightforward depiction of a person at a podium. There are no clear indications of stereotypes or oversimplification of complex issues in the image.</p> </li> </ul>"},{"location":"dataset.md/#article-2-las-graffiti-tagged-skyscraper-a-work-of-art-and-symbol-of-citys-wider-failings-the-guardian-us","title":"Article 2: LA\u2019s graffiti-tagged skyscraper: a work of art \u2013 and symbol of city\u2019s wider failings - The Guardian US","text":"<ul> <li>Unique ID: 1148232027</li> <li>Title: LA\u2019s graffiti-tagged skyscraper: a work of art \u2013 and symbol of city\u2019s wider failings - The Guardian US</li> <li>Text:  <p>An asparagus patch is how the architect Charles Moore described the lackluster skyline of downtown Los Angeles in the 1980s. \"The tallest stalk and the shortest stalk are just alike, except that the tallest has shot farther out of the ground.\" This sprawling city of bungalows has never been known for the quality of its high-rise buildings, and not much has changed since Moore\u2019s day. A 1950s ordinance dictating that every tower must have a flat roof was rescinded in 2014, spawning a handful of clumsy quiffs and crowns atop a fresh crop of swollen glass slabs. It only added further evidence to the notion that architects in this seismic city are probably better suited to staying on the ground.</p> </li> <li>Outlet: The Guardian US</li> <li>Source URL: The Guardian US</li> <li>Topics: affordable housing, public housing, homeowners, housing crisis</li> <li>Date Published: 2024-03-17</li> <li>Image Description:  <p>The image shows a tall, multi-story building with numerous windows. The building is covered in various graffiti tags and symbols, with words like 'READY', 'SHAKA', 'RAKM', 'TOOL', 'TOLT', 'KERZ', 'SMK', 'DZER', 'MSK', and 'OBER' prominently displayed. The building is situated in an urban environment with other structures visible in the background. The sky is clear, suggesting it might be daytime. The image is taken from a high angle, looking down on the building.</p> </li> <li>Text Label: Likely</li> <li>Text Bias Analysis: <p>\"mind-numbingly generic glass boxes\", \"abandoned\", \"doing nothing\", \"if they ain\u2019t gon finish the job\", \"This building has needed love for years\", \"the streets of LA are happy to make something out of it\", \"the developer had ceased paying\"</p> </li> <li>Image Label: Likely</li> <li>Image Analysis: <p>The image and accompanying headline from The Guardian US suggest a critical perspective on the state of urban development and the impact of graffiti on architecture. The choice of this image may be intended to highlight the issue of urban decay and the lack of maintenance in certain areas. The graffiti tags could be seen as a form of artistic expression, but within the context of the headline, they are likely to be interpreted as a symbol of the city's wider failings. The image does not provide a balanced view, as it focuses on the negative aspects of the building's appearance. The framing of the image, with the building as the central focus and the surrounding environment in the background, may lead viewers to associate the building's condition with the overall state of the city.</p> </li> </ul>"}]}